{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9603490,"sourceType":"datasetVersion","datasetId":5859126},{"sourceId":9878904,"sourceType":"datasetVersion","datasetId":6065253}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install imblearn\n\nimport numpy as np\nimport itertools\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import IsolationForest\nfrom scipy.stats import boxcox\nfrom sklearn.preprocessing import RobustScaler\nfrom imblearn.over_sampling import SMOTENC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom functools import partial\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom sklearn.manifold import TSNE\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\nfrom io import BytesIO\nimport base64\nimport seaborn as sns\nfrom joblib import Parallel, delayed\n\nwarnings.filterwarnings(\"ignore\")\n\n\ndf = pd.read_csv('/kaggle/input/kaggle-s4e10/playground-series-s4e10/train.csv')\ndf_origin = pd.read_csv('/kaggle/input/ps4e9-original-data-loan-approval-prediction/credit_risk_dataset.csv')\n\ndf_origin['person_emp_length'].fillna(df_origin['person_emp_length'].mean(), inplace=True)\ndf_origin['loan_int_rate'].fillna(df_origin['loan_int_rate'].mean(), inplace=True)\n\n\ndf = df.drop(columns=['id'])\n\ndf = pd.concat([df, df_origin],axis=0)\n\n\nnumeric_var = ['person_age', 'person_income', 'person_emp_length', 'loan_amnt', 'loan_int_rate', 'loan_percent_income', 'cb_person_cred_hist_length']","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n        tf.config.experimental.set_memory_growth(gpus[0], True)\n    except RuntimeError as e:\n        print(e)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Isolation Forest","metadata":{}},{"cell_type":"code","source":"def isol_forest(df):\n\n    anomaly_inputs = numeric_var\n    model_IF = IsolationForest(contamination=0.05, random_state=42)\n    model_IF.fit(df[anomaly_inputs])\n    df['anomaly_scores'] = model_IF.decision_function(df[anomaly_inputs])\n    df['anomaly'] = model_IF.predict(df[anomaly_inputs])\n\n    df = df[df['anomaly'] == 1].reset_index(drop=True)\n    df.drop(['anomaly_scores', 'anomaly'], axis=1, inplace=True)\n    \n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Interquartile Range (IQR)","metadata":{}},{"cell_type":"code","source":"def iqr(df):\n\n    for col in numeric_var:\n        \n        Q1 = df[col].quantile(0.25)\n        Q3 = df[col].quantile(0.75)\n        IQR = Q3 - Q1\n        \n        lower_bound = Q1 - 2 * IQR\n        upper_bound = Q3 + 2 * IQR\n        \n        df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n\n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Box-Cox Transformation","metadata":{}},{"cell_type":"code","source":"def box_cox(df):\n    for col in numeric_var:\n        df[col], _ = boxcox(df[col] + 1)\n\n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Robust Scaler","metadata":{}},{"cell_type":"code","source":"def robust_scaler(df):\n    scaler = RobustScaler()\n    df[numeric_var] = scaler.fit_transform(df[numeric_var])\n\n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Normalization","metadata":{}},{"cell_type":"code","source":"def normalization(df):\n    for col in numeric_var:\n        df[col] = (df[col] - df[col].min()) / (df[col].max() - df[col].min())\n\n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Standardization","metadata":{}},{"cell_type":"code","source":"def standardization(df):\n    for col in numeric_var:\n        df[col] = (df[col] - df[col].mean()) / df[col].std()\n\n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# SMOTENC","metadata":{}},{"cell_type":"code","source":"def smotenc(df, sampling_strategy=0.5):\n    X = df[['person_age', 'person_income', 'person_home_ownership', 'person_emp_length', 'loan_intent', 'loan_grade', 'loan_amnt', 'loan_int_rate', 'loan_percent_income', 'cb_person_default_on_file', 'cb_person_cred_hist_length']]\n    y = df[['loan_status']]\n    \n    categorical_features = [2, 4, 5, 9]\n\n    smotenc = SMOTENC(sampling_strategy=sampling_strategy, categorical_features=categorical_features, random_state=42)\n    \n    X_resampled, y_resampled = smotenc.fit_resample(X, y)\n    \n    X_resampled_df = pd.DataFrame(X_resampled, columns=['person_age', 'person_income', 'person_home_ownership', 'person_emp_length', 'loan_intent', 'loan_grade', 'loan_amnt', 'loan_int_rate', 'loan_percent_income', 'cb_person_default_on_file', 'cb_person_cred_hist_length'])\n    y_resampled_df = pd.DataFrame(y_resampled, columns=['loan_status'])\n    df = pd.concat([X_resampled_df, y_resampled_df], axis=1)\n    \n    return df\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# One-Hot Encoding","metadata":{}},{"cell_type":"code","source":"def one_hot(df):\n    df = pd.get_dummies(df, columns=['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file'])\n\n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Label Encoding","metadata":{}},{"cell_type":"code","source":"def label_enc(df):\n    label_encoder = LabelEncoder()\n    \n    for column in ['person_home_ownership', 'loan_intent', 'loan_grade', 'cb_person_default_on_file']:\n        df[column] = label_encoder.fit_transform(df[column])\n\n    return df","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"def Model_and_save(df_train, df_val, function, save_function):\n\n    class NeuralNetworkModel:\n        def __init__(self, input_dim, dropout_rate=0.2, patience=5, batchnorm=True, random_seed=42):\n\n            self.input_dim = input_dim\n            self.dropout_rate = dropout_rate\n            self.patience = patience\n            self.batchnorm = batchnorm\n            \n            self.random_seed = random_seed\n            tf.random.set_seed(self.random_seed)\n            \n            self.model = self.build_model()\n    \n            self.early_stopping = EarlyStopping(monitor='val_auc', patience=self.patience, restore_best_weights=True)\n    \n        def build_model(self):\n\n            model = Sequential()\n    \n            model.add(Dense(64, input_dim=self.input_dim, activation='relu'))\n    \n            if self.batchnorm:\n                model.add(BatchNormalization())\n    \n            model.add(Dense(128, activation='relu'))\n            model.add(Dropout(self.dropout_rate))\n    \n            if self.batchnorm:\n                model.add(BatchNormalization())\n                \n            model.add(Dense(64, activation='relu'))\n            model.add(Dropout(self.dropout_rate))\n\n            model.add(Dense(1, activation='sigmoid'))\n\n            lr_schedule = ExponentialDecay(\n            initial_learning_rate=0.0005,  \n            decay_steps=1000,            \n            decay_rate=0.85,            \n            staircase=False             \n            )\n    \n            model.compile(optimizer=Adam(learning_rate=lr_schedule), \n                          loss='binary_crossentropy', \n                          metrics=['accuracy', tf.keras.metrics.AUC(name='auc')])\n    \n            return model\n    \n        def train(self, X_train, y_train, X_val, y_val, epochs=50, batch_size=32):\n\n            history = self.model.fit(X_train, y_train,\n                                     validation_data=(X_val, y_val),\n                                     epochs=epochs,\n                                     batch_size=batch_size,\n                                     callbacks=[self.early_stopping],\n                                     verbose=0)\n            return history\n    \n        def evaluate(self, X_val, y_val):\n\n            val_loss, val_accuracy, val_auc = self.model.evaluate(X_val, y_val)\n            return val_loss, val_accuracy, val_auc\n\n        def predict(self, X):\n            return self.model.predict(X)\n            \n    X_train = df_train.drop(columns=['loan_status'])\n    y_train = df_train['loan_status'] \n\n    X_val = df_val.drop(columns=['loan_status'])\n    y_val = df_val['loan_status'] \n    \n    with tf.device('/GPU:0'):\n        nn_model = NeuralNetworkModel(input_dim=X_train.shape[1], dropout_rate=function[-2], patience=30, batchnorm=function[-1])\n        history = nn_model.train(X_train, y_train, X_val, y_val, epochs=100, batch_size=512)\n        \n        y_pred = nn_model.predict(X_val)\n    \n        tsne = TSNE(n_components=2, random_state=42)\n        X_tsne = tsne.fit_transform(X_val)\n        \n        fig, ax = plt.subplots(figsize=(8, 6))\n        \n        scatter = ax.scatter(X_tsne[:, 0], X_tsne[:, 1], c=y_pred, cmap='viridis', edgecolor='k', s=50)\n        plt.colorbar(scatter, ax=ax)\n        ax.set_title(\"t-SNE of Test Data (Colored by Model Predictions)\")\n        ax.set_xlabel(\"t-SNE Component 1\")\n        ax.set_ylabel(\"t-SNE Component 2\")\n        \n\n        buf = BytesIO()\n        fig.savefig(buf, format='png', dpi=100, transparent=False)\n        buf.seek(0)  \n        plt.close(fig)  \n        \n        image_binary_tsne = buf.getvalue()\n        encoded_image_tsne = base64.b64encode(image_binary_tsne).decode('utf-8')\n        buf.close()\n    \n        y_pred_binary = (y_pred > 0.5).astype(int)\n        \n        cm = confusion_matrix(y_val, y_pred_binary)\n        class_names = [\"0\", \"1\"]\n        \n        fig, ax = plt.subplots(figsize=(8, 6))\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names, ax=ax)\n        ax.set_title(\"Confusion Matrix\")\n        ax.set_xlabel(\"Predicted Label\")\n        ax.set_ylabel(\"True Label\")\n        \n        buf = BytesIO()\n        fig.savefig(buf, format='png', dpi=100, transparent=False)\n        buf.seek(0)\n        plt.close(fig)\n        \n        image_binary_conf = buf.getvalue()\n        encoded_image_conf = base64.b64encode(image_binary_conf).decode('utf-8')\n        buf.close()\n        image_binary = base64.b64decode(encoded_image_conf)\n    \n    \n        df_save = pd.DataFrame([{\n        'isol_forest': save_function[0],\n        'iqr': save_function[1],\n        'box_cox': save_function[2],\n        'robust_scaler': save_function[3],\n        'normalization': save_function[4],\n        'SMOTENC': save_function[5],\n        'one_hot': save_function[6],\n        'Dropout': save_function[7],\n        'BatchNorm': save_function[8],\n        'history': history.history,\n        't-SNE': encoded_image_tsne,\n        'confusion_matrix': encoded_image_conf\n    }])\n        \n        file_path = \"info.csv\"\n        \n        def save_to_csv(df_save, file_path):\n            try:\n                if not pd.io.common.file_exists(file_path):\n                    df_save.to_csv(file_path, index=False)\n                else:\n                    df_save.to_csv(file_path, mode='a', index=False, header=False)\n            except Exception as e:\n                print(f\"Error saving to CSV: {e}\")\n        \n        save_to_csv(df_save, file_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Main","metadata":{}},{"cell_type":"code","source":"def main(combination, save_combination):\n    df_new = df.copy()\n    df_train, df_val = train_test_split(df_new, test_size=0.2, random_state=42)\n    \n    for i, func in enumerate(combination):\n        if callable(func):            \n            df_train = func(df_train)\n            if i != 5:\n                df_val = func(df_val)\n\n    Model_and_save(df_train, df_val, combination, save_combination)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"functions = [[False, isol_forest], [False, iqr], [False, box_cox], [False, robust_scaler], [False, standardization, normalization], \\\n              [False, lambda df: smotenc(df, sampling_strategy=0.4), lambda df: smotenc(df, sampling_strategy=0.6),  \\\n              lambda df: smotenc(df, sampling_strategy=0.8)], [label_enc, one_hot], [0, 0.15, 0.25, 0.4], [False, True]]\n\nsave_functions = [[False, True], [False, True], [False, True], [False, True], [False, 'standardization', 'normalization'], \\\n                  [False, 0.4, 0.6, 0.8], ['label_enc', 'one_hot'], [0, 0.15, 0.25, 0.4], [False, True]]\n\ncombinations = list(itertools.product(*functions))\nsave_combinations = list(itertools.product(*save_functions))\n\nParallel(n_jobs=3, backend=\"threading\")(\n    delayed(main)(combination, save_combination) \n    for combination, save_combination in zip(combinations, save_combinations)\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}